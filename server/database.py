# database.py

import logging
import sqlite3
import mysql.connector
import psycopg2
import json
import os
from datetime import datetime
from psycopg2.extras import RealDictCursor

# 从项目根目录导入模块
from config import SITES_DATA_FILE, config_manager

# 外部库导入
from qbittorrentapi import Client
from transmission_rpc import Client as TrClient

# --- [重要修正] ---
# 直接从 core.services 导入正确的函数，移除了会导致错误的 try-except 占位符
from core.services import _prepare_api_config

# 导入数据库迁移管理模块
from database_migrations import DatabaseMigrationManager


class DatabaseManager:
    """处理与配置的数据库（MySQL、PostgreSQL 或 SQLite）的所有交互。"""

    def __init__(self, config):
        """根据提供的配置初始化 DatabaseManager。"""
        self.db_type = config.get("db_type", "sqlite")
        if self.db_type == "mysql":
            self.mysql_config = config.get("mysql", {})
            logging.info("数据库后端设置为 MySQL。")
            # 自动创建数据库（如果不存在）
            self._ensure_database_exists()
        elif self.db_type == "postgresql":
            self.postgresql_config = config.get("postgresql", {})
            logging.info("数据库后端设置为 PostgreSQL。")
            # 自动创建数据库（如果不存在）
            self._ensure_database_exists()
        else:
            self.sqlite_path = config.get("path", "data/pt_stats.db")
            logging.info(f"数据库后端设置为 SQLite。路径: {self.sqlite_path}")
            # SQLite 会自动创建文件，无需额外处理

        # 初始化迁移管理器
        self.migration_manager = DatabaseMigrationManager(self)

    def _ensure_database_exists(self):
        """确保数据库存在，如果不存在则自动创建。"""
        if self.db_type == "mysql":
            database_name = self.mysql_config.get("database")
            if not database_name:
                logging.error("MySQL 配置中缺少 database 参数")
                return

            try:
                # 连接到 MySQL 服务器（不指定数据库）
                conn_config = self.mysql_config.copy()
                target_db = conn_config.pop("database")

                conn = mysql.connector.connect(**conn_config, autocommit=True)
                cursor = conn.cursor()

                # 检查数据库是否存在
                cursor.execute(
                    "SELECT SCHEMA_NAME FROM INFORMATION_SCHEMA.SCHEMATA WHERE SCHEMA_NAME = %s",
                    (target_db, ))

                if cursor.fetchone() is None:
                    # 数据库不存在，创建它
                    logging.info(f"数据库 '{target_db}' 不存在，正在创建...")
                    cursor.execute(
                        f"CREATE DATABASE `{target_db}` CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci"
                    )
                    logging.info(f"✓ 成功创建数据库 '{target_db}'")
                else:
                    logging.info(f"数据库 '{target_db}' 已存在")

                cursor.close()
                conn.close()

            except Exception as e:
                logging.error(f"创建 MySQL 数据库时出错: {e}", exc_info=True)
                raise

        elif self.db_type == "postgresql":
            database_name = self.postgresql_config.get("database")
            if not database_name:
                logging.error("PostgreSQL 配置中缺少 database 参数")
                return

            try:
                # 连接到 PostgreSQL 服务器的 postgres 默认数据库
                conn_config = self.postgresql_config.copy()
                target_db = conn_config.pop("database")
                conn_config["database"] = "postgres"

                conn = psycopg2.connect(**conn_config)
                conn.autocommit = True
                cursor = conn.cursor()

                # 检查数据库是否存在
                cursor.execute("SELECT 1 FROM pg_database WHERE datname = %s",
                               (target_db, ))

                if cursor.fetchone() is None:
                    # 数据库不存在，创建它
                    logging.info(f"数据库 '{target_db}' 不存在，正在创建...")
                    cursor.execute(
                        f'CREATE DATABASE "{target_db}" ENCODING \'UTF8\'')
                    logging.info(f"✓ 成功创建数据库 '{target_db}'")
                else:
                    logging.info(f"数据库 '{target_db}' 已存在")

                cursor.close()
                conn.close()

            except Exception as e:
                logging.error(f"创建 PostgreSQL 数据库时出错: {e}", exc_info=True)
                raise

    def _get_connection(self):
        """返回一个新的数据库连接。"""
        if self.db_type == "mysql":
            # 添加字符集配置以避免字符集冲突
            mysql_config = self.mysql_config.copy()
            mysql_config['charset'] = 'utf8mb4'
            mysql_config['collation'] = 'utf8mb4_unicode_ci'
            return mysql.connector.connect(**mysql_config,
                                           autocommit=False)
        elif self.db_type == "postgresql":
            return psycopg2.connect(**self.postgresql_config)
        else:
            return sqlite3.connect(self.sqlite_path, timeout=20)

    def _get_cursor(self, conn):
        """从连接中返回一个游标。"""
        if self.db_type == "mysql":
            return conn.cursor(dictionary=True, buffered=True)
        elif self.db_type == "postgresql":
            return conn.cursor(cursor_factory=RealDictCursor)
        else:
            conn.row_factory = sqlite3.Row
            return conn.cursor()

    def get_placeholder(self):
        """返回数据库类型对应的正确参数占位符。"""
        return "%s" if self.db_type in ["mysql", "postgresql"] else "?"

    def get_site_by_nickname(self, nickname):
        """通过站点昵称从数据库中获取站点的完整信息。"""
        conn = self._get_connection()
        cursor = self._get_cursor(conn)
        try:
            # 首先尝试通过nickname查询
            cursor.execute(
                f"SELECT * FROM sites WHERE nickname = {self.get_placeholder()}",
                (nickname, ))
            site_data = cursor.fetchone()

            # 如果通过nickname没有找到，尝试通过site字段查询
            if not site_data:
                cursor.execute(
                    f"SELECT * FROM sites WHERE site = {self.get_placeholder()}",
                    (nickname, ))
                site_data = cursor.fetchone()

            return dict(site_data) if site_data else None
        except Exception as e:
            logging.error(f"通过昵称 '{nickname}' 获取站点信息时出错: {e}")
            return None
        finally:
            cursor.close()
            conn.close()

    def add_site(self, site_data):
        """向数据库中添加一个新站点。"""
        conn = self._get_connection()
        cursor = self._get_cursor(conn)
        ph = self.get_placeholder()
        try:
            # 根据数据库类型使用正确的标识符引用符
            if self.db_type == "postgresql":
                sql = f"INSERT INTO sites (site, nickname, base_url, special_tracker_domain, \"group\", description, cookie, passkey, speed_limit, ratio_threshold, seed_speed_limit) VALUES ({ph}, {ph}, {ph}, {ph}, {ph}, {ph}, {ph}, {ph}, {ph}, {ph}, {ph})"
            else:
                sql = f"INSERT INTO sites (site, nickname, base_url, special_tracker_domain, `group`, description, cookie, passkey, speed_limit, ratio_threshold, seed_speed_limit) VALUES ({ph}, {ph}, {ph}, {ph}, {ph}, {ph}, {ph}, {ph}, {ph}, {ph}, {ph})"
            # 去除cookie字符串首尾的换行符和多余空白字符
            cookie = site_data.get("cookie")
            if cookie:
                cookie = cookie.strip()

            ratio_threshold = site_data.get("ratio_threshold")
            if ratio_threshold in (None, ""):
                ratio_threshold = 3.0
            else:
                ratio_threshold = float(ratio_threshold)

            seed_speed_limit = site_data.get("seed_speed_limit")
            if seed_speed_limit in (None, ""):
                seed_speed_limit = 5
            else:
                seed_speed_limit = int(seed_speed_limit)

            params = (
                site_data.get("site"),
                site_data.get("nickname"),
                site_data.get("base_url"),
                site_data.get("special_tracker_domain"),
                site_data.get("group"),
                site_data.get("description"),
                cookie,
                site_data.get("passkey"),
                int(site_data.get("speed_limit", 0)),
                ratio_threshold,
                seed_speed_limit,
            )
            cursor.execute(sql, params)
            conn.commit()
            return True
        except Exception as e:
            if "UNIQUE constraint failed" in str(
                    e) or "Duplicate entry" in str(e):
                logging.error(f"添加站点失败：站点域名 '{site_data.get('site')}' 已存在。")
            else:
                logging.error(f"添加站点 '{site_data.get('nickname')}' 失败: {e}",
                              exc_info=True)
            conn.rollback()
            return False
        finally:
            cursor.close()
            conn.close()

    def update_site_details(self, site_data):
        """根据站点 ID 更新其所有可编辑的详细信息。"""
        conn = self._get_connection()
        cursor = self._get_cursor(conn)
        ph = self.get_placeholder()
        try:
            # 根据数据库类型使用正确的标识符引用符
            if self.db_type == "postgresql":
                sql = f"UPDATE sites SET nickname = {ph}, base_url = {ph}, special_tracker_domain = {ph}, \"group\" = {ph}, description = {ph}, cookie = {ph}, passkey = {ph}, speed_limit = {ph}, ratio_threshold = {ph}, seed_speed_limit = {ph} WHERE id = {ph}"
            else:
                sql = f"UPDATE sites SET nickname = {ph}, base_url = {ph}, special_tracker_domain = {ph}, `group` = {ph}, description = {ph}, cookie = {ph}, passkey = {ph}, speed_limit = {ph}, ratio_threshold = {ph}, seed_speed_limit = {ph} WHERE id = {ph}"
            # 去除cookie字符串首尾的换行符和多余空白字符
            cookie = site_data.get("cookie")
            if cookie:
                cookie = cookie.strip()

            ratio_threshold = site_data.get("ratio_threshold")
            if ratio_threshold in (None, ""):
                ratio_threshold = 3.0
            else:
                ratio_threshold = float(ratio_threshold)
                if ratio_threshold <= 0:
                    ratio_threshold = 3.0

            seed_speed_limit = site_data.get("seed_speed_limit")
            if seed_speed_limit in (None, ""):
                seed_speed_limit = 5
            else:
                seed_speed_limit = int(seed_speed_limit)

            params = (
                site_data.get("nickname"),
                site_data.get("base_url"),
                site_data.get("special_tracker_domain"),
                site_data.get("group"),
                site_data.get("description"),
                cookie,
                site_data.get("passkey"),
                int(site_data.get("speed_limit", 0)),
                ratio_threshold,
                seed_speed_limit,
                site_data.get("id"),
            )
            cursor.execute(sql, params)
            conn.commit()
            return cursor.rowcount > 0
        except Exception as e:
            logging.error(f"更新站点ID '{site_data.get('id')}' 失败: {e}",
                          exc_info=True)
            conn.rollback()
            return False
        finally:
            cursor.close()
            conn.close()

    def delete_site(self, site_id):
        """根据站点 ID 从数据库中删除一个站点，并将其记录到配置文件的已删除列表中。"""
        conn = self._get_connection()
        cursor = self._get_cursor(conn)
        try:
            # 先查询站点的 site 标识符
            cursor.execute(
                f"SELECT site FROM sites WHERE id = {self.get_placeholder()}",
                (site_id, ))
            site_row = cursor.fetchone()

            if not site_row:
                return False

            site_identifier = site_row["site"] if isinstance(
                site_row, dict) else site_row[0]

            # 删除站点
            cursor.execute(
                f"DELETE FROM sites WHERE id = {self.get_placeholder()}",
                (site_id, ))
            conn.commit()

            if cursor.rowcount > 0 and site_identifier:
                # 将站点标识符添加到配置文件的 deleted_sites 列表中
                current_config = config_manager.get()
                deleted_sites = current_config.get("deleted_sites", [])

                if site_identifier not in deleted_sites:
                    deleted_sites.append(site_identifier)
                    current_config["deleted_sites"] = deleted_sites
                    config_manager.save(current_config)
                    logging.info(f"已将站点 '{site_identifier}' 添加到已删除列表")

                return True

            return cursor.rowcount > 0
        except Exception as e:
            logging.error(f"删除站点ID '{site_id}' 失败: {e}", exc_info=True)
            conn.rollback()
            return False
        finally:
            cursor.close()
            conn.close()

    def update_site_cookie(self, nickname, cookie):
        """按昵称更新指定站点的 Cookie (主要由CookieCloud使用)。"""
        conn = self._get_connection()
        cursor = self._get_cursor(conn)
        try:
            # 去除cookie字符串首尾的换行符和多余空白字符
            if cookie:
                cookie = cookie.strip()
            cursor.execute(
                f"UPDATE sites SET cookie = {self.get_placeholder()} WHERE nickname = {self.get_placeholder()}",
                (cookie, nickname),
            )
            conn.commit()
            return cursor.rowcount > 0
        except Exception as e:
            logging.error(f"更新站点 '{nickname}' 的 Cookie 失败: {e}", exc_info=True)
            conn.rollback()
            return False
        finally:
            cursor.close()
            conn.close()

    def sync_sites_from_json(self):
        """从 sites_data.json 同步站点数据到数据库，过滤已删除的站点。"""
        try:
            # 读取 JSON 文件
            with open(SITES_DATA_FILE, 'r', encoding='utf-8') as f:
                sites_data = json.load(f)

            logging.info(f"从 {SITES_DATA_FILE} 加载了 {len(sites_data)} 个站点")

            # 获取已删除的站点列表
            deleted_sites = config_manager.get().get("deleted_sites", [])
            if deleted_sites:
                logging.info(
                    f"过滤 {len(deleted_sites)} 个已删除的站点: {deleted_sites}")
                # 过滤掉已删除的站点
                sites_data = [
                    site for site in sites_data
                    if site.get('site') not in deleted_sites
                ]
                logging.info(f"过滤后剩余 {len(sites_data)} 个站点")

            # 获取数据库连接
            conn = self._get_connection()
            cursor = self._get_cursor(conn)

            try:
                # [修改] 查询时额外获取 speed_limit 和 passkey 字段，用于后续逻辑判断
                cursor.execute(
                    "SELECT id, site, nickname, base_url, speed_limit, passkey, ratio_threshold, seed_speed_limit FROM sites"
                )
                existing_sites = {}
                for row in cursor.fetchall():
                    # 以site、nickname、base_url为键存储现有站点
                    # 将整行数据存起来，方便后续获取 speed_limit
                    row_dict = dict(row)
                    existing_sites[row['site']] = row_dict
                    if row['nickname']:
                        existing_sites[row['nickname']] = row_dict
                    if row['base_url']:
                        existing_sites[row['base_url']] = row_dict

                updated_count = 0
                added_count = 0

                # 遍历 JSON 中的站点数据
                for site_info in sites_data:
                    site_name = site_info.get('site')
                    nickname = site_info.get('nickname')
                    base_url = site_info.get('base_url')

                    if not site_name or not nickname or not base_url:
                        logging.warning(f"跳过无效的站点数据: {site_info}")
                        continue

                    # 检查站点是否已存在（基于site、nickname或base_url中的任何一个）
                    existing_site = None
                    if site_name in existing_sites:
                        existing_site = existing_sites[site_name]
                    elif nickname in existing_sites:
                        existing_site = existing_sites[nickname]
                    elif base_url in existing_sites:
                        existing_site = existing_sites[base_url]

                    if existing_site:
                        # --- [核心修改逻辑] ---
                        # 获取数据库中当前的 speed_limit
                        db_speed_limit = existing_site.get('speed_limit', 0)
                        # 获取 JSON 文件中的 speed_limit
                        json_speed_limit = site_info.get('speed_limit', 0)

                        # 默认使用数据库中现有的值
                        final_speed_limit = db_speed_limit

                        # 如果数据库值为0，且JSON值不为0，则采纳JSON的值
                        if db_speed_limit == 0 and json_speed_limit != 0:
                            final_speed_limit = json_speed_limit

                        # 获取数据库中当前的 passkey
                        db_passkey = existing_site.get('passkey', '')
                        # 获取 JSON 文件中的 passkey
                        json_passkey = site_info.get('passkey', '')

                        # 默认使用数据库中现有的值，保护已设置的 passkey
                        final_passkey = db_passkey

                        # 只有当数据库中为空且JSON中有值时，才使用JSON的值
                        if not db_passkey and json_passkey:
                            final_passkey = json_passkey
                            logging.debug(f"为站点 '{site_name}' 设置新的 passkey")
                        elif db_passkey:
                            logging.debug(f"保护站点 '{site_name}' 的现有 passkey")

                        db_ratio_threshold = existing_site.get('ratio_threshold')
                        json_ratio_threshold = site_info.get('ratio_threshold')
                        final_ratio_threshold = db_ratio_threshold

                        # 保护用户在数据库中的手动配置：仅当数据库为空或非法时才尝试用JSON/默认值回填
                        if final_ratio_threshold is None or float(final_ratio_threshold) <= 0:
                            if json_ratio_threshold is not None and float(json_ratio_threshold) > 0:
                                final_ratio_threshold = float(json_ratio_threshold)
                            else:
                                final_ratio_threshold = 3.0

                        db_seed_speed_limit = existing_site.get('seed_speed_limit')
                        json_seed_speed_limit = site_info.get('seed_speed_limit')
                        final_seed_speed_limit = db_seed_speed_limit

                        # 保护用户在数据库中的手动配置：仅当数据库为空或非法时才尝试用JSON/默认值回填
                        if final_seed_speed_limit is None or int(final_seed_speed_limit) < 0:
                            if json_seed_speed_limit is not None and int(json_seed_speed_limit) >= 0:
                                final_seed_speed_limit = int(json_seed_speed_limit)
                            else:
                                final_seed_speed_limit = 5
                        # --- [核心修改逻辑结束] ---

                        # 构建更新语句，不包含 cookie，使用保护后的 passkey
                        if self.db_type == "postgresql":
                            update_sql = """
                                UPDATE sites
                                SET site = %s, nickname = %s, base_url = %s, special_tracker_domain = %s,
                                    "group" = %s, description = %s, passkey = %s, migration = %s, speed_limit = %s,
                                    ratio_threshold = %s, seed_speed_limit = %s
                                WHERE id = %s
                            """
                        elif self.db_type == "mysql":
                            update_sql = """
                                UPDATE sites
                                SET site = %s, nickname = %s, base_url = %s, special_tracker_domain = %s,
                                    `group` = %s, description = %s, passkey = %s, migration = %s, speed_limit = %s,
                                    ratio_threshold = %s, seed_speed_limit = %s
                                WHERE id = %s
                            """
                        else:  # SQLite
                            update_sql = """
                                UPDATE sites
                                SET site = ?, nickname = ?, base_url = ?, special_tracker_domain = ?,
                                    "group" = ?, description = ?, passkey = ?, migration = ?, speed_limit = ?,
                                    ratio_threshold = ?, seed_speed_limit = ?
                                WHERE id = ?
                            """

                        # 执行更新，传入经过逻辑判断后的 final_speed_limit 和 final_passkey
                        cursor.execute(
                            update_sql,
                            (
                                site_info.get('site'),
                                site_info.get('nickname'),
                                site_info.get('base_url'),
                                site_info.get('special_tracker_domain'),
                                site_info.get('group'),
                                site_info.get('description'),
                                final_passkey,  # 使用保护后的 passkey 值
                                site_info.get('migration', 0),
                                final_speed_limit,  # 使用条件判断后的最终值
                                final_ratio_threshold,
                                final_seed_speed_limit,
                                existing_site['id']))
                        updated_count += 1
                        logging.debug(f"更新了站点: {site_name}")
                    else:
                        # 根据数据库类型使用正确的标识符引用符和占位符
                        if self.db_type == "postgresql":
                            # 添加新站点
                            cursor.execute(
                                """
                                INSERT INTO sites
                                (site, nickname, base_url, special_tracker_domain, "group", description, passkey, migration, speed_limit, ratio_threshold, seed_speed_limit)
                                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                            """, (site_info.get('site'),
                                  site_info.get('nickname'),
                                  site_info.get('base_url'),
                                  site_info.get('special_tracker_domain'),
                                  site_info.get('group'),
                                  site_info.get('description'),
                                  site_info.get('passkey'),
                                  site_info.get('migration', 0),
                                  site_info.get('speed_limit', 0),
                                  site_info.get('ratio_threshold', 3.0),
                                  site_info.get('seed_speed_limit', 5)))
                        elif self.db_type == "mysql":
                            # 添加新站点
                            cursor.execute(
                                """
                                INSERT INTO sites
                                (site, nickname, base_url, special_tracker_domain, `group`, description, passkey, migration, speed_limit, ratio_threshold, seed_speed_limit)
                                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                            """, (site_info.get('site'),
                                  site_info.get('nickname'),
                                  site_info.get('base_url'),
                                  site_info.get('special_tracker_domain'),
                                  site_info.get('group'),
                                  site_info.get('description'),
                                  site_info.get('passkey'),
                                  site_info.get('migration', 0),
                                  site_info.get('speed_limit', 0),
                                  site_info.get('ratio_threshold', 3.0),
                                  site_info.get('seed_speed_limit', 5)))
                        else:  # SQLite
                            # 添加新站点
                            cursor.execute(
                                """
                                INSERT INTO sites
                                (site, nickname, base_url, special_tracker_domain, "group", description, passkey, migration, speed_limit, ratio_threshold, seed_speed_limit)
                                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                            """, (site_info.get('site'),
                                  site_info.get('nickname'),
                                  site_info.get('base_url'),
                                  site_info.get('special_tracker_domain'),
                                  site_info.get('group'),
                                  site_info.get('description'),
                                  site_info.get('passkey'),
                                  site_info.get('migration', 0),
                                  site_info.get('speed_limit', 0),
                                  site_info.get('ratio_threshold', 3.0),
                                  site_info.get('seed_speed_limit', 5)))
                        added_count += 1
                        logging.debug(f"添加了新站点: {site_name}")

                conn.commit()
                logging.info(f"站点同步完成: {updated_count} 个更新, {added_count} 个新增")
                return True

            except Exception as e:
                conn.rollback()
                logging.error(f"同步站点数据时出错: {e}", exc_info=True)
                return False
            finally:
                if cursor:
                    cursor.close()
                if conn:
                    conn.close()

        except Exception as e:
            logging.error(f"读取站点数据文件时出错: {e}", exc_info=True)
            return False

    def init_db(self):
        """确保数据库表存在，并根据 sites_data.json 同步站点数据。"""
        conn = self._get_connection()
        cursor = self._get_cursor(conn)

        logging.info("正在初始化并验证数据库表结构...")
        # 表创建逻辑 (MySQL)
        if self.db_type == "mysql":
            cursor.execute(
                "CREATE TABLE IF NOT EXISTS traffic_stats (stat_datetime DATETIME NOT NULL, downloader_id VARCHAR(36) NOT NULL, uploaded BIGINT DEFAULT 0, downloaded BIGINT DEFAULT 0, upload_speed BIGINT DEFAULT 0, download_speed BIGINT DEFAULT 0, cumulative_uploaded BIGINT NOT NULL DEFAULT 0, cumulative_downloaded BIGINT NOT NULL DEFAULT 0, PRIMARY KEY (stat_datetime, downloader_id)) ENGINE=InnoDB ROW_FORMAT=Dynamic"
            )
            # 创建小时聚合表 (MySQL)
            cursor.execute(
                "CREATE TABLE IF NOT EXISTS traffic_stats_hourly (stat_datetime DATETIME NOT NULL, downloader_id VARCHAR(36) NOT NULL, uploaded BIGINT DEFAULT 0, downloaded BIGINT DEFAULT 0, avg_upload_speed BIGINT DEFAULT 0, avg_download_speed BIGINT DEFAULT 0, samples INTEGER DEFAULT 0, cumulative_uploaded BIGINT NOT NULL DEFAULT 0, cumulative_downloaded BIGINT NOT NULL DEFAULT 0, PRIMARY KEY (stat_datetime, downloader_id)) ENGINE=InnoDB ROW_FORMAT=Dynamic"
            )
            cursor.execute(
                "CREATE TABLE IF NOT EXISTS torrents (hash VARCHAR(40) NOT NULL, name TEXT NOT NULL, save_path TEXT, size BIGINT, progress FLOAT, state VARCHAR(50), sites VARCHAR(255), `group` VARCHAR(255), details TEXT, downloader_id VARCHAR(36) NOT NULL, last_seen DATETIME NOT NULL, iyuu_last_check DATETIME NULL, seeders INT DEFAULT 0, PRIMARY KEY (hash, downloader_id)) ENGINE=InnoDB ROW_FORMAT=Dynamic"
            )
            cursor.execute(
                "CREATE TABLE IF NOT EXISTS torrent_upload_stats (hash VARCHAR(40) NOT NULL, downloader_id VARCHAR(36) NOT NULL, uploaded BIGINT DEFAULT 0, PRIMARY KEY (hash, downloader_id)) ENGINE=InnoDB ROW_FORMAT=Dynamic"
            )
            cursor.execute(
                "CREATE TABLE IF NOT EXISTS `sites` (`id` mediumint NOT NULL AUTO_INCREMENT, `site` varchar(255) UNIQUE DEFAULT NULL, `nickname` varchar(255) DEFAULT NULL, `base_url` varchar(255) DEFAULT NULL, `special_tracker_domain` varchar(255) DEFAULT NULL, `group` varchar(255) DEFAULT NULL, `description` varchar(255) DEFAULT NULL, `cookie` TEXT DEFAULT NULL, `passkey` TEXT DEFAULT NULL, `migration` int(11) NOT NULL DEFAULT 1, `speed_limit` int(11) NOT NULL DEFAULT 0, `ratio_threshold` REAL NOT NULL DEFAULT 3.0, `seed_speed_limit` int(11) NOT NULL DEFAULT 5, PRIMARY KEY (`id`)) ENGINE=InnoDB ROW_FORMAT=DYNAMIC"
            )
            # 创建种子参数表，用于存储从源站点提取的种子参数
            cursor.execute(
                "CREATE TABLE IF NOT EXISTS seed_parameters (hash VARCHAR(40) NOT NULL, torrent_id VARCHAR(255) NOT NULL, site_name VARCHAR(255) NOT NULL, nickname VARCHAR(255), name TEXT, title TEXT, subtitle TEXT, imdb_link TEXT, douban_link TEXT, type VARCHAR(100), medium VARCHAR(100), video_codec VARCHAR(100), audio_codec VARCHAR(100), resolution VARCHAR(100), team VARCHAR(100), source VARCHAR(100), tags TEXT, poster TEXT, screenshots TEXT, statement TEXT, body TEXT, mediainfo TEXT, title_components TEXT, removed_ardtudeclarations TEXT, is_reviewed TINYINT(1) NOT NULL DEFAULT 0, mediainfo_status VARCHAR(20) DEFAULT 'pending', bdinfo_task_id VARCHAR(36), bdinfo_started_at DATETIME, bdinfo_completed_at DATETIME, bdinfo_error TEXT, created_at DATETIME NOT NULL, updated_at DATETIME NOT NULL, PRIMARY KEY (hash, torrent_id, site_name)) ENGINE=InnoDB ROW_FORMAT=DYNAMIC"
            )
            # 创建批量转种记录表
            cursor.execute(
                "CREATE TABLE IF NOT EXISTS batch_enhance_records (id INT AUTO_INCREMENT PRIMARY KEY, title TEXT, batch_id VARCHAR(255) NOT NULL, torrent_id VARCHAR(255) NOT NULL, source_site VARCHAR(255) NOT NULL, target_site VARCHAR(255) NOT NULL, video_size_gb DECIMAL(8,2), status VARCHAR(50) NOT NULL, success_url TEXT, error_detail TEXT, downloader_add_result TEXT, processed_at DATETIME DEFAULT CURRENT_TIMESTAMP, progress VARCHAR(20), INDEX idx_batch_records_batch_id (batch_id), INDEX idx_batch_records_torrent_id (torrent_id), INDEX idx_batch_records_status (status), INDEX idx_batch_records_processed_at (processed_at)) ENGINE=InnoDB ROW_FORMAT=DYNAMIC"
            )
        # 表创建逻辑 (PostgreSQL)
        elif self.db_type == "postgresql":
            cursor.execute(
                "CREATE TABLE IF NOT EXISTS traffic_stats (stat_datetime TIMESTAMP NOT NULL, downloader_id VARCHAR(36) NOT NULL, uploaded BIGINT DEFAULT 0, downloaded BIGINT DEFAULT 0, upload_speed BIGINT DEFAULT 0, download_speed BIGINT DEFAULT 0, cumulative_uploaded BIGINT NOT NULL DEFAULT 0, cumulative_downloaded BIGINT NOT NULL DEFAULT 0, PRIMARY KEY (stat_datetime, downloader_id))"
            )
            # 创建小时聚合表 (PostgreSQL)
            cursor.execute(
                "CREATE TABLE IF NOT EXISTS traffic_stats_hourly (stat_datetime TIMESTAMP NOT NULL, downloader_id VARCHAR(36) NOT NULL, uploaded BIGINT DEFAULT 0, downloaded BIGINT DEFAULT 0, avg_upload_speed BIGINT DEFAULT 0, avg_download_speed BIGINT DEFAULT 0, samples INTEGER DEFAULT 0, cumulative_uploaded BIGINT NOT NULL DEFAULT 0, cumulative_downloaded BIGINT NOT NULL DEFAULT 0, PRIMARY KEY (stat_datetime, downloader_id))"
            )
            cursor.execute(
                "CREATE TABLE IF NOT EXISTS torrents (hash VARCHAR(40) NOT NULL, name TEXT NOT NULL, save_path TEXT, size BIGINT, progress REAL, state VARCHAR(50), sites VARCHAR(255), \"group\" VARCHAR(255), details TEXT, downloader_id VARCHAR(36) NOT NULL, last_seen TIMESTAMP NOT NULL, iyuu_last_check TIMESTAMP NULL, seeders INTEGER DEFAULT 0, PRIMARY KEY (hash, downloader_id))"
            )
            cursor.execute(
                "CREATE TABLE IF NOT EXISTS torrent_upload_stats (hash VARCHAR(40) NOT NULL, downloader_id VARCHAR(36) NOT NULL, uploaded BIGINT DEFAULT 0, PRIMARY KEY (hash, downloader_id))"
            )
            cursor.execute(
                "CREATE TABLE IF NOT EXISTS sites (id SERIAL PRIMARY KEY, site VARCHAR(255) UNIQUE, nickname VARCHAR(255), base_url VARCHAR(255), special_tracker_domain VARCHAR(255), \"group\" VARCHAR(255), description VARCHAR(255), cookie TEXT, passkey TEXT, migration INTEGER NOT NULL DEFAULT 1, speed_limit INTEGER NOT NULL DEFAULT 0, ratio_threshold REAL NOT NULL DEFAULT 3.0, seed_speed_limit INTEGER NOT NULL DEFAULT 5)"
            )
            # 创建种子参数表，用于存储从源站点提取的种子参数
            cursor.execute(
                "CREATE TABLE IF NOT EXISTS seed_parameters (hash VARCHAR(40) NOT NULL, torrent_id VARCHAR(255) NOT NULL, site_name VARCHAR(255) NOT NULL, nickname VARCHAR(255), name TEXT, title TEXT, subtitle TEXT, imdb_link TEXT, douban_link TEXT, type VARCHAR(100), medium VARCHAR(100), video_codec VARCHAR(100), audio_codec VARCHAR(100), resolution VARCHAR(100), team VARCHAR(100), source VARCHAR(100), tags TEXT, poster TEXT, screenshots TEXT, statement TEXT, body TEXT, mediainfo TEXT, title_components TEXT, removed_ardtudeclarations TEXT, is_reviewed BOOLEAN NOT NULL DEFAULT FALSE, mediainfo_status VARCHAR(20) DEFAULT 'pending', bdinfo_task_id VARCHAR(36), bdinfo_started_at TIMESTAMP, bdinfo_completed_at TIMESTAMP, bdinfo_error TEXT, created_at TIMESTAMP NOT NULL, updated_at TIMESTAMP NOT NULL, PRIMARY KEY (hash, torrent_id, site_name))"
            )
            # 创建批量转种记录表
            cursor.execute(
                "CREATE TABLE IF NOT EXISTS batch_enhance_records (id SERIAL PRIMARY KEY, title TEXT, batch_id VARCHAR(255) NOT NULL, torrent_id VARCHAR(255) NOT NULL, source_site VARCHAR(255) NOT NULL, target_site VARCHAR(255) NOT NULL, video_size_gb DECIMAL(8,2), status VARCHAR(50) NOT NULL, success_url TEXT, error_detail TEXT, downloader_add_result TEXT, processed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP, progress VARCHAR(20))"
            )
            cursor.execute(
                "CREATE INDEX IF NOT EXISTS idx_batch_records_batch_id ON batch_enhance_records(batch_id)"
            )
            cursor.execute(
                "CREATE INDEX IF NOT EXISTS idx_batch_records_torrent_id ON batch_enhance_records(torrent_id)"
            )
            cursor.execute(
                "CREATE INDEX IF NOT EXISTS idx_batch_records_status ON batch_enhance_records(status)"
            )
            cursor.execute(
                "CREATE INDEX IF NOT EXISTS idx_batch_records_processed_at ON batch_enhance_records(processed_at)"
            )
        # 表创建逻辑 (SQLite)
        else:
            cursor.execute(
                "CREATE TABLE IF NOT EXISTS traffic_stats (stat_datetime TEXT NOT NULL, downloader_id TEXT NOT NULL, uploaded INTEGER DEFAULT 0, downloaded INTEGER DEFAULT 0, upload_speed INTEGER DEFAULT 0, download_speed INTEGER DEFAULT 0, cumulative_uploaded INTEGER NOT NULL DEFAULT 0, cumulative_downloaded INTEGER NOT NULL DEFAULT 0, PRIMARY KEY (stat_datetime, downloader_id))"
            )
            # 创建小时聚合表 (SQLite)
            cursor.execute(
                "CREATE TABLE IF NOT EXISTS traffic_stats_hourly (stat_datetime TEXT NOT NULL, downloader_id TEXT NOT NULL, uploaded INTEGER DEFAULT 0, downloaded INTEGER DEFAULT 0, avg_upload_speed INTEGER DEFAULT 0, avg_download_speed INTEGER DEFAULT 0, samples INTEGER DEFAULT 0, cumulative_uploaded INTEGER NOT NULL DEFAULT 0, cumulative_downloaded INTEGER NOT NULL DEFAULT 0, PRIMARY KEY (stat_datetime, downloader_id))"
            )
            cursor.execute(
                "CREATE TABLE IF NOT EXISTS torrents (hash TEXT NOT NULL, name TEXT NOT NULL, save_path TEXT, size INTEGER, progress REAL, state TEXT, sites TEXT, `group` TEXT, details TEXT, downloader_id TEXT NOT NULL, last_seen TEXT NOT NULL, iyuu_last_check TEXT NULL, seeders INTEGER DEFAULT 0, PRIMARY KEY (hash, downloader_id))"
            )
            cursor.execute(
                "CREATE TABLE IF NOT EXISTS torrent_upload_stats (hash TEXT NOT NULL, downloader_id TEXT NOT NULL, uploaded INTEGER DEFAULT 0, PRIMARY KEY (hash, downloader_id))"
            )
            cursor.execute(
                "CREATE TABLE IF NOT EXISTS sites (id INTEGER PRIMARY KEY AUTOINCREMENT, site TEXT UNIQUE, nickname TEXT, base_url TEXT, special_tracker_domain TEXT, `group` TEXT, description TEXT, cookie TEXT, passkey TEXT, migration INTEGER NOT NULL DEFAULT 1, speed_limit INTEGER NOT NULL DEFAULT 0, ratio_threshold REAL NOT NULL DEFAULT 3.0, seed_speed_limit INTEGER NOT NULL DEFAULT 5)"
            )
            # 创建种子参数表，用于存储从源站点提取的种子参数
            cursor.execute(
                "CREATE TABLE IF NOT EXISTS seed_parameters (hash TEXT NOT NULL, torrent_id TEXT NOT NULL, site_name TEXT NOT NULL, nickname TEXT, name TEXT, title TEXT, subtitle TEXT, imdb_link TEXT, douban_link TEXT, type TEXT, medium TEXT, video_codec TEXT, audio_codec TEXT, resolution TEXT, team TEXT, source TEXT, tags TEXT, poster TEXT, screenshots TEXT, statement TEXT, body TEXT, mediainfo TEXT, title_components TEXT, removed_ardtudeclarations TEXT, is_reviewed INTEGER NOT NULL DEFAULT 0, mediainfo_status TEXT DEFAULT 'pending', bdinfo_task_id TEXT, bdinfo_started_at TEXT, bdinfo_completed_at TEXT, bdinfo_error TEXT, created_at TEXT NOT NULL, updated_at TEXT NOT NULL, PRIMARY KEY (hash, torrent_id, site_name))"
            )
            # 创建批量转种记录表
            cursor.execute(
                "CREATE TABLE IF NOT EXISTS batch_enhance_records (id INTEGER PRIMARY KEY AUTOINCREMENT, title TEXT, batch_id TEXT NOT NULL, torrent_id TEXT NOT NULL, source_site TEXT NOT NULL, target_site TEXT NOT NULL, video_size_gb REAL, status TEXT NOT NULL, success_url TEXT, error_detail TEXT, downloader_add_result TEXT, processed_at TEXT DEFAULT CURRENT_TIMESTAMP, progress TEXT)"
            )
            cursor.execute(
                "CREATE INDEX IF NOT EXISTS idx_batch_records_batch_id ON batch_enhance_records(batch_id)"
            )
            cursor.execute(
                "CREATE INDEX IF NOT EXISTS idx_batch_records_torrent_id ON batch_enhance_records(torrent_id)"
            )
            cursor.execute(
                "CREATE INDEX IF NOT EXISTS idx_batch_records_status ON batch_enhance_records(status)"
            )
            cursor.execute(
                "CREATE INDEX IF NOT EXISTS idx_batch_records_processed_at ON batch_enhance_records(processed_at)"
            )

        conn.commit()

        # 使用统一的迁移管理器执行所有迁移检查
        logging.info("开始执行数据库迁移检查...")
        migration_success = self.migration_manager.run_all_migrations(conn, cursor)

        if migration_success:
            logging.info("✓ 所有数据库迁移检查完成")
        else:
            logging.warning("数据库迁移过程中出现警告，但系统仍可继续运行")

        # 同步站点数据
        self.sync_sites_from_json()

    def aggregate_hourly_traffic(self, retention_hours=48):
        """
        聚合小时流量数据并清理原始数据。
        
        此函数是数据聚合策略的核心，它将 traffic_stats 表中的原始数据按小时聚合到 
        traffic_stats_hourly 表中，然后删除已聚合的原始数据以控制数据库大小。
        
        Args:
            retention_hours (int): 保留原始数据的时间（小时）。
                                  在此时间之前的原始数据将被聚合和删除。
        """
        from datetime import datetime, timedelta

        # 计算聚合和清理的边界时间
        cutoff_time = datetime.now() - timedelta(hours=retention_hours)

        # 添加特殊日期保护逻辑
        # 确保不会聚合最近3天的数据，以防止数据丢失
        # 修改为按日计算，聚合到三天前的00:00:00
        now = datetime.now()
        safe_cutoff = (now - timedelta(days=3)).replace(hour=0,
                                                        minute=0,
                                                        second=0,
                                                        microsecond=0)
        if cutoff_time > safe_cutoff:
            logging.info(f"为防止数据丢失，调整聚合截止时间为 {safe_cutoff}")
            cutoff_time = safe_cutoff

        cutoff_time_str = cutoff_time.strftime("%Y-%m-%d %H:%M:%S")
        ph = self.get_placeholder()

        conn = None
        cursor = None

        try:
            conn = self._get_connection()
            cursor = self._get_cursor(conn)

            # 开始事务
            if self.db_type == "postgresql":
                # PostgreSQL 需要显式开始事务
                cursor.execute("BEGIN")

            # 根据数据库类型生成时间截断函数
            if self.db_type == "mysql":
                time_group_fn = "DATE_FORMAT(stat_datetime, '%Y-%m-%d %H:00:00')"
            elif self.db_type == "postgresql":
                time_group_fn = "DATE_TRUNC('hour', stat_datetime)"
            else:  # sqlite
                time_group_fn = "STRFTIME('%Y-%m-%d %H:00:00', stat_datetime)"

            # 执行聚合查询：从原始表中按小时分组计算聚合值
            # 对于累计存储方式，我们需要计算每个时间段的累计值差值作为该时间段的流量
            if self.db_type == "postgresql":
                aggregate_query = f"""
                    SELECT
                        {time_group_fn} AS hour_group,
                        downloader_id,
                        GREATEST(0, (MAX(cumulative_uploaded) - MIN(cumulative_uploaded))::bigint) AS total_uploaded,
                        GREATEST(0, (MAX(cumulative_downloaded) - MIN(cumulative_downloaded))::bigint) AS total_downloaded,
                        AVG(upload_speed) AS avg_upload_speed,
                        AVG(download_speed) AS avg_download_speed,
                        COUNT(*) AS samples
                    FROM traffic_stats
                    WHERE stat_datetime < {ph}
                    GROUP BY hour_group, downloader_id
                """
            elif self.db_type == "mysql":
                aggregate_query = f"""
                    SELECT
                        {time_group_fn} AS hour_group,
                        downloader_id,
                        GREATEST(0, MAX(cumulative_uploaded) - MIN(cumulative_uploaded)) AS total_uploaded,
                        GREATEST(0, MAX(cumulative_downloaded) - MIN(cumulative_downloaded)) AS total_downloaded,
                        AVG(upload_speed) AS avg_upload_speed,
                        AVG(download_speed) AS avg_download_speed,
                        COUNT(*) AS samples
                    FROM traffic_stats
                    WHERE stat_datetime < {ph}
                    GROUP BY hour_group, downloader_id
                """
            else:  # SQLite - 使用CASE语句替代GREATEST函数
                aggregate_query = f"""
                    SELECT
                        {time_group_fn} AS hour_group,
                        downloader_id,
                        CASE WHEN MAX(cumulative_uploaded) - MIN(cumulative_uploaded) > 0 
                             THEN MAX(cumulative_uploaded) - MIN(cumulative_uploaded) 
                             ELSE 0 END AS total_uploaded,
                        CASE WHEN MAX(cumulative_downloaded) - MIN(cumulative_downloaded) > 0 
                             THEN MAX(cumulative_downloaded) - MIN(cumulative_downloaded) 
                             ELSE 0 END AS total_downloaded,
                        AVG(upload_speed) AS avg_upload_speed,
                        AVG(download_speed) AS avg_download_speed,
                        COUNT(*) AS samples
                    FROM traffic_stats
                    WHERE stat_datetime < {ph}
                    GROUP BY hour_group, downloader_id
                """

            cursor.execute(aggregate_query, (cutoff_time_str, ))
            aggregated_rows = cursor.fetchall()

            # 如果没有数据需要聚合，则直接返回
            if not aggregated_rows:
                logging.info("没有需要聚合的数据。")
                conn.commit()
                return

            # 批量插入聚合数据到 traffic_stats_hourly 表中
            # 使用 UPSERT 机制处理重复数据
            if self.db_type == "mysql":
                upsert_sql = f"""
                    INSERT INTO traffic_stats_hourly
                    (stat_datetime, downloader_id, uploaded, downloaded, avg_upload_speed, avg_download_speed, samples, cumulative_uploaded, cumulative_downloaded)
                    VALUES ({ph}, {ph}, {ph}, {ph}, {ph}, {ph}, {ph}, {ph}, {ph})
                    ON DUPLICATE KEY UPDATE
                    uploaded = uploaded + VALUES(uploaded),
                    downloaded = downloaded + VALUES(downloaded),
                    avg_upload_speed = ((avg_upload_speed * samples) + (VALUES(avg_upload_speed) * VALUES(samples))) / (samples + VALUES(samples)),
                    avg_download_speed = ((avg_download_speed * samples) + (VALUES(avg_download_speed) * VALUES(samples))) / (samples + VALUES(samples)),
                    samples = samples + VALUES(samples),
                    cumulative_uploaded = GREATEST(cumulative_uploaded, VALUES(cumulative_uploaded)),
                    cumulative_downloaded = GREATEST(cumulative_downloaded, VALUES(cumulative_downloaded))
                """
            elif self.db_type == "postgresql":
                upsert_sql = f"""
                    INSERT INTO traffic_stats_hourly
                    (stat_datetime, downloader_id, uploaded, downloaded, avg_upload_speed, avg_download_speed, samples, cumulative_uploaded, cumulative_downloaded)
                    VALUES ({ph}, {ph}, {ph}, {ph}, {ph}, {ph}, {ph}, {ph}, {ph})
                    ON CONFLICT (stat_datetime, downloader_id)
                    DO UPDATE SET
                    uploaded = traffic_stats_hourly.uploaded + EXCLUDED.uploaded,
                    downloaded = traffic_stats_hourly.downloaded + EXCLUDED.downloaded,
                    avg_upload_speed = ((traffic_stats_hourly.avg_upload_speed * traffic_stats_hourly.samples) + (EXCLUDED.avg_upload_speed * EXCLUDED.samples)) / (traffic_stats_hourly.samples + EXCLUDED.samples),
                    avg_download_speed = ((traffic_stats_hourly.avg_download_speed * traffic_stats_hourly.samples) + (EXCLUDED.avg_download_speed * EXCLUDED.samples)) / (traffic_stats_hourly.samples + EXCLUDED.samples),
                    samples = traffic_stats_hourly.samples + EXCLUDED.samples,
                    cumulative_uploaded = GREATEST(traffic_stats_hourly.cumulative_uploaded, EXCLUDED.cumulative_uploaded),
                    cumulative_downloaded = GREATEST(traffic_stats_hourly.cumulative_downloaded, EXCLUDED.cumulative_downloaded)
                """
            else:  # sqlite
                upsert_sql = f"""
                    INSERT INTO traffic_stats_hourly
                    (stat_datetime, downloader_id, uploaded, downloaded, avg_upload_speed, avg_download_speed, samples, cumulative_uploaded, cumulative_downloaded)
                    VALUES ({ph}, {ph}, {ph}, {ph}, {ph}, {ph}, {ph}, {ph}, {ph})
                    ON CONFLICT (stat_datetime, downloader_id)
                    DO UPDATE SET
                    uploaded = traffic_stats_hourly.uploaded + excluded.uploaded,
                    downloaded = traffic_stats_hourly.downloaded + excluded.downloaded,
                    avg_upload_speed = ((traffic_stats_hourly.avg_upload_speed * traffic_stats_hourly.samples) + (excluded.avg_upload_speed * excluded.samples)) / (traffic_stats_hourly.samples + excluded.samples),
                    avg_download_speed = ((traffic_stats_hourly.avg_download_speed * traffic_stats_hourly.samples) + (excluded.avg_download_speed * excluded.samples)) / (traffic_stats_hourly.samples + excluded.samples),
                    samples = traffic_stats_hourly.samples + excluded.samples,
                    cumulative_uploaded = MAX(traffic_stats_hourly.cumulative_uploaded, excluded.cumulative_uploaded),
                    cumulative_downloaded = MAX(traffic_stats_hourly.cumulative_downloaded, excluded.cumulative_downloaded)
                """

            # 准备插入参数
            # 对于累计值，我们需要获取每个时间段内最后一个记录的累计值
            # 重新查询以获取累计值
            if self.db_type == "mysql":
                time_group_fn = "DATE_FORMAT(stat_datetime, '%Y-%m-%d %H:00:00')"
            elif self.db_type == "postgresql":
                time_group_fn = "DATE_TRUNC('hour', stat_datetime)"
            else:  # sqlite
                time_group_fn = "STRFTIME('%Y-%m-%d %H:00:00', stat_datetime)"

            cumulative_query = f"""
                SELECT
                    {time_group_fn} AS hour_group,
                    downloader_id,
                    MAX(cumulative_uploaded) AS final_cumulative_uploaded,
                    MAX(cumulative_downloaded) AS final_cumulative_downloaded
                FROM traffic_stats
                WHERE stat_datetime < {ph}
                GROUP BY hour_group, downloader_id
            """

            cursor.execute(cumulative_query, (cutoff_time_str, ))
            cumulative_rows = cursor.fetchall()

            # 创建累计值映射字典
            cumulative_map = {}
            for row in cumulative_rows:
                key = (row["hour_group"] if isinstance(row, dict) else row[0],
                       row["downloader_id"]
                       if isinstance(row, dict) else row[1])
                cumulative_map[key] = (
                    int(row["final_cumulative_uploaded"] if isinstance(
                        row, dict) else row[2]),
                    int(row["final_cumulative_downloaded"] if isinstance(
                        row, dict) else row[3]))

            # 准备插入参数
            upsert_params = [
                (row["hour_group"] if isinstance(row, dict) else row[0],
                 row["downloader_id"] if isinstance(row, dict) else row[1],
                 int(row["total_uploaded"] if isinstance(row, dict) else row[2]
                     ),
                 int(row["total_downloaded"] if isinstance(row, dict
                                                           ) else row[3]),
                 int(row["avg_upload_speed"] if isinstance(row, dict
                                                           ) else row[4]),
                 int(row["avg_download_speed"] if isinstance(row, dict
                                                             ) else row[5]),
                 int(row["samples"] if isinstance(row, dict) else row[6]),
                 cumulative_map.get(
                     (row["hour_group"] if isinstance(row, dict) else row[0],
                      row["downloader_id"]
                      if isinstance(row, dict) else row[1]), (0, 0))[0],
                 cumulative_map.get(
                     (row["hour_group"] if isinstance(row, dict) else row[0],
                      row["downloader_id"]
                      if isinstance(row, dict) else row[1]), (0, 0))[1])
                for row in aggregated_rows
            ]

            cursor.executemany(upsert_sql, upsert_params)

            # 删除已聚合的原始数据
            delete_query = f"DELETE FROM traffic_stats WHERE stat_datetime < {ph}"
            cursor.execute(delete_query, (cutoff_time_str, ))

            # 提交事务
            conn.commit()

            logging.info(
                f"成功聚合 {len(aggregated_rows)} 条小时数据，并清理了 {cursor.rowcount} 条原始数据。"
            )
        except Exception as e:
            # 回滚事务
            if conn:
                conn.rollback()
            logging.error(f"聚合小时流量数据时出错: {e}", exc_info=True)
            raise
        finally:
            # 关闭游标和连接
            if cursor:
                cursor.close()
            if conn:
                conn.close()

def reconcile_historical_data(db_manager, config):
    """在启动时同步下载器状态到数据库。"""
    # MySQL BIGINT 有符号最大值
    MAX_BIGINT = 9223372036854775807

    logging.info("正在同步下载器状态...")
    conn = db_manager._get_connection()
    cursor = db_manager._get_cursor(conn)
    ph = db_manager.get_placeholder()

    records = []
    current_timestamp_str = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    for client_config in config.get("downloaders", []):
        if not client_config.get("enabled"):
            continue
        client_id = client_config["id"]
        try:
            total_dl, total_ul = 0, 0
            if client_config["type"] == "qbittorrent":
                api_config = _prepare_api_config(client_config)
                client = Client(**api_config)
                client.auth_log_in()
                server_state = client.sync_maindata().get('server_state', {})
                total_dl = int(server_state.get('alltime_dl', 0))
                total_ul = int(server_state.get('alltime_ul', 0))
            elif client_config["type"] == "transmission":
                api_config = _prepare_api_config(client_config)
                client = TrClient(**api_config)
                stats = client.session_stats()
                total_dl = int(stats.cumulative_stats.downloaded_bytes)
                total_ul = int(stats.cumulative_stats.uploaded_bytes)

            # --- 修复溢出问题的逻辑 ---
            # 确保数值不超过 MySQL BIGINT 的最大值，且不为负数
            if total_dl > MAX_BIGINT:
                logging.warning(f"下载器 {client_config['name']} 的累计下载量过大 ({total_dl})，已截断为最大允许值。")
                total_dl = MAX_BIGINT
            if total_ul > MAX_BIGINT:
                logging.warning(f"下载器 {client_config['name']} 的累计上传量过大 ({total_ul})，已截断为最大允许值。")
                total_ul = MAX_BIGINT

            # 确保不为负数（防止某些客户端 API 返回负值）
            total_dl = max(0, total_dl)
            total_ul = max(0, total_ul)
            # ------------------------

            records.append((current_timestamp_str, client_id, 0, 0, 0, 0,
                            total_ul, total_dl))
            logging.info(f"客户端 '{client_config['name']}' 的状态已同步。")
        except Exception as e:
            logging.error(f"[{client_config['name']}] 状态同步失败: {e}")

    if records:
        try:
            sql_insert = (
                f"INSERT INTO traffic_stats (stat_datetime, downloader_id, uploaded, downloaded, upload_speed, download_speed, cumulative_uploaded, cumulative_downloaded) VALUES ({ph}, {ph}, {ph}, {ph}, {ph}, {ph}, {ph}, {ph}) ON CONFLICT(stat_datetime, downloader_id) DO UPDATE SET uploaded = EXCLUDED.uploaded, downloaded = EXCLUDED.downloaded, cumulative_uploaded = EXCLUDED.cumulative_uploaded, cumulative_downloaded = EXCLUDED.cumulative_downloaded"
                if db_manager.db_type == "postgresql" else
                f"INSERT INTO traffic_stats (stat_datetime, downloader_id, uploaded, downloaded, upload_speed, download_speed, cumulative_uploaded, cumulative_downloaded) VALUES ({ph}, {ph}, {ph}, {ph}, {ph}, {ph}, {ph}, {ph}) ON DUPLICATE KEY UPDATE uploaded = VALUES(uploaded), downloaded = VALUES(downloaded), cumulative_uploaded = VALUES(cumulative_uploaded), cumulative_downloaded = VALUES(cumulative_downloaded)"
                if db_manager.db_type == "mysql" else
                f"INSERT INTO traffic_stats (stat_datetime, downloader_id, uploaded, downloaded, upload_speed, download_speed, cumulative_uploaded, cumulative_downloaded) VALUES (?, ?, ?, ?, ?, ?, ?, ?) ON CONFLICT(stat_datetime, downloader_id) DO UPDATE SET uploaded = excluded.uploaded, downloaded = excluded.downloaded, cumulative_uploaded = excluded.cumulative_uploaded, cumulative_downloaded = excluded.cumulative_downloaded"
            )
            cursor.executemany(sql_insert, records)
            logging.info(f"已成功插入 {len(records)} 条初始记录到 traffic_stats。")
        except Exception as e:
            logging.error(f"插入初始记录失败: {e}")
            conn.rollback()

    conn.commit()
    cursor.close()
    conn.close()
